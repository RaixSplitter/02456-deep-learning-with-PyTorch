{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZW0gaQO81Sq"
   },
   "source": [
    "# Transfer learning on the Caltech101 dataset\n",
    "\n",
    "In this notebook, we will consider a more complex dataset than MNIST or CIFAR10. The images in Caltech101 are RGB images (3 channels) with variable size. There are 101 different classes. We will try a very common practice in computer vision nowadays: transfer learning from a pre-trained ImageNet model. \n",
    "\n",
    "Roadmap:\n",
    "- Modify the network from the previous exercise (CIFAR-10) to work with 224x224 images.\n",
    "- Train the model for a while on Caltech101 and see how far we can get.\n",
    "- Take a ResNet34 that was pre-trained on ImageNet-1k and fine-tune it to Caltech101.\n",
    "  - Consider both training only the head (the linear classifier at the end of the network) or the entire network.\n",
    "  - We should be able to reach better performance than our original network in fewer training steps.\n",
    "- Optional: play around with other pre-trained models from `timm` (see info [here](https://github.com/rwightman/pytorch-image-models)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htyg7xxN81St"
   },
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v3u2GIWr81Su",
    "outputId": "4bbb197c-5bf8-4f82-f74d-28fcddc175be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm\n",
      "  Downloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n",
      "     ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.0/2.3 MB 653.6 kB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.1/2.3 MB 1.4 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 0.3/2.3 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 0.6/2.3 MB 3.2 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 1.1/2.3 MB 4.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 2.1/2.3 MB 7.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.3/2.3 MB 7.8 MB/s eta 0:00:00\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.25.0-py3-none-any.whl (436 kB)\n",
      "     ---------------------------------------- 0.0/436.4 kB ? eta -:--:--\n",
      "     ------------------------------------- 436.4/436.4 kB 26.6 MB/s eta 0:00:00\n",
      "Collecting pyyaml\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-win_amd64.whl (161 kB)\n",
      "     ---------------------------------------- 0.0/161.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 161.8/161.8 kB ? eta 0:00:00\n",
      "Collecting safetensors\n",
      "  Downloading safetensors-0.4.5-cp310-none-win_amd64.whl (285 kB)\n",
      "     ---------------------------------------- 0.0/285.9 kB ? eta -:--:--\n",
      "     ------------------------------------- 285.9/285.9 kB 18.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torchvision in c:\\programmering\\dtu\\02456-deep-learning-with-pytorch\\.venv\\lib\\site-packages (from timm) (0.19.1)\n",
      "Requirement already satisfied: torch in c:\\programmering\\dtu\\02456-deep-learning-with-pytorch\\.venv\\lib\\site-packages (from timm) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\programmering\\dtu\\02456-deep-learning-with-pytorch\\.venv\\lib\\site-packages (from huggingface_hub->timm) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programmering\\dtu\\02456-deep-learning-with-pytorch\\.venv\\lib\\site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programmering\\dtu\\02456-deep-learning-with-pytorch\\.venv\\lib\\site-packages (from huggingface_hub->timm) (2024.9.0)\n",
      "Requirement already satisfied: filelock in c:\\programmering\\dtu\\02456-deep-learning-with-pytorch\\.venv\\lib\\site-packages (from huggingface_hub->timm) (3.16.1)\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "     ---------------------------------------- 0.0/64.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 64.9/64.9 kB ? eta 0:00:00\n",
      "Collecting tqdm>=4.42.1\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 0.0/78.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 78.4/78.4 kB ? eta 0:00:00\n",
      "Requirement already satisfied: networkx in c:\\programmering\\dtu\\02456-deep-learning-with-pytorch\\.venv\\lib\\site-packages (from torch->timm) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programmering\\dtu\\02456-deep-learning-with-pytorch\\.venv\\lib\\site-packages (from torch->timm) (3.1.4)\n",
      "Requirement already satisfied: sympy in c:\\programmering\\dtu\\02456-deep-learning-with-pytorch\\.venv\\lib\\site-packages (from torch->timm) (1.13.3)\n",
      "Requirement already satisfied: numpy in c:\\programmering\\dtu\\02456-deep-learning-with-pytorch\\.venv\\lib\\site-packages (from torchvision->timm) (2.1.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programmering\\dtu\\02456-deep-learning-with-pytorch\\.venv\\lib\\site-packages (from torchvision->timm) (10.4.0)\n",
      "Requirement already satisfied: colorama in c:\\programmering\\dtu\\02456-deep-learning-with-pytorch\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programmering\\dtu\\02456-deep-learning-with-pytorch\\.venv\\lib\\site-packages (from jinja2->torch->timm) (2.1.5)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl (100 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "     ---------------------------------------- 0.0/167.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 167.3/167.3 kB ? eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "     ---------------------------------------- 0.0/70.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 70.4/70.4 kB ? eta 0:00:00\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "     ---------------------------------------- 0.0/126.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 126.3/126.3 kB ? eta 0:00:00\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programmering\\dtu\\02456-deep-learning-with-pytorch\\.venv\\lib\\site-packages (from sympy->torch->timm) (1.3.0)\n",
      "Installing collected packages: urllib3, tqdm, safetensors, pyyaml, idna, charset-normalizer, certifi, requests, huggingface_hub, timm\n",
      "Successfully installed certifi-2024.8.30 charset-normalizer-3.3.2 huggingface_hub-0.25.0 idna-3.10 pyyaml-6.0.2 requests-2.32.3 safetensors-0.4.5 timm-1.0.9 tqdm-4.66.5 urllib3-2.2.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "c:\\programmering\\DTU\\02456-deep-learning-with-PyTorch\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from typing import List, Optional, Callable, Iterator\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "!pip install timm\n",
    "import timm\n",
    "\n",
    "def accuracy(target, pred):\n",
    "    return accuracy_score(target.detach().cpu().numpy(), pred.detach().cpu().numpy())\n",
    "\n",
    "def show_image(img, title=None):\n",
    "    img = img.detach().cpu()\n",
    "    img = img.permute((1, 2, 0)).numpy()\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean   # unnormalize\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.gca().tick_params(axis=\"both\", which=\"both\", bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRJYSCs5BUOf"
   },
   "source": [
    "### Set up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZeTujLC81S3",
    "outputId": "b46495bc-cfc3-4f0b-cf5e-db4fc38cb372"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "To download files from GDrive, 'gdown' is required. You can install it with 'pip install gdown'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\programmering\\DTU\\02456-deep-learning-with-PyTorch\\.venv\\lib\\site-packages\\torchvision\\datasets\\utils.py:193\u001b[0m, in \u001b[0;36mdownload_file_from_google_drive\u001b[1;34m(file_id, root, filename, md5)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgdown\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gdown'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m  \u001b[38;5;66;03m# both for training and testing\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Load dataset (downloaded automatically if missing).\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCaltech101\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslate_label\u001b[39m(y, keep_classes):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\programmering\\DTU\\02456-deep-learning-with-PyTorch\\.venv\\lib\\site-packages\\torchvision\\datasets\\caltech.py:55\u001b[0m, in \u001b[0;36mCaltech101.__init__\u001b[1;34m(self, root, target_type, transform, target_transform, download)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_type \u001b[38;5;241m=\u001b[39m [verify_str_arg(t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotation\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m target_type]\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\programmering\\DTU\\02456-deep-learning-with-PyTorch\\.venv\\lib\\site-packages\\torchvision\\datasets\\caltech.py:136\u001b[0m, in \u001b[0;36mCaltech101.download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiles already downloaded and verified\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://drive.google.com/file/d/137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m101_ObjectCategories.tar.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmd5\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb224c7392d521a49829488ab0f1120d9\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m download_and_extract_archive(\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://drive.google.com/file/d/175kQy3UsZ0wUEHZjqkUDdNVssr7bgh_m\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[0;32m    145\u001b[0m     filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnnotations.tar\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    146\u001b[0m     md5\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m6f83eeb1f24d99cab4eb377263132c91\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    147\u001b[0m )\n",
      "File \u001b[1;32mc:\\programmering\\DTU\\02456-deep-learning-with-PyTorch\\.venv\\lib\\site-packages\\torchvision\\datasets\\utils.py:395\u001b[0m, in \u001b[0;36mdownload_and_extract_archive\u001b[1;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[0;32m    393\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[1;32m--> 395\u001b[0m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    397\u001b[0m archive \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_root, filename)\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marchive\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_root\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\programmering\\DTU\\02456-deep-learning-with-PyTorch\\.venv\\lib\\site-packages\\torchvision\\datasets\\utils.py:127\u001b[0m, in \u001b[0;36mdownload_url\u001b[1;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[0;32m    125\u001b[0m file_id \u001b[38;5;241m=\u001b[39m _get_google_drive_file_id(url)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdownload_file_from_google_drive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# download the file\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\programmering\\DTU\\02456-deep-learning-with-PyTorch\\.venv\\lib\\site-packages\\torchvision\\datasets\\utils.py:195\u001b[0m, in \u001b[0;36mdownload_file_from_google_drive\u001b[1;34m(file_id, root, filename, md5)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgdown\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[1;32m--> 195\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo download files from GDrive, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgdown\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is required. You can install it with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install gdown\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m     )\n\u001b[0;32m    199\u001b[0m root \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(root)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: To download files from GDrive, 'gdown' is required. You can install it with 'pip install gdown'."
     ]
    }
   ],
   "source": [
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "default_imagenet_normalization = transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.25, 1)),  # crop of random size and aspect ratio, resize to square\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    default_imagenet_normalization,\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),  # keep aspect ratio\n",
    "    transforms.CenterCrop(224),  # square crop\n",
    "    transforms.ToTensor(),\n",
    "    default_imagenet_normalization,\n",
    "])\n",
    "\n",
    "batch_size = 64  # both for training and testing\n",
    "\n",
    "# Load dataset (downloaded automatically if missing).\n",
    "dataset = torchvision.datasets.Caltech101(root='./data', download=True)\n",
    "\n",
    "\n",
    "def translate_label(y, keep_classes):\n",
    "    try:\n",
    "        return keep_classes.index(y)\n",
    "    except ValueError:\n",
    "        return -1\n",
    "\n",
    "\n",
    "# Filter images such that no class has more than 100 examples.\n",
    "# Loop through dataset in random order, include each image (its index) only if there are \n",
    "# fewer than 100 images of the same class.\n",
    "# Here we also remove the classes \"Faces\" and \"Faces_easy\".\n",
    "skip_classes = [0, 1]   # \"Faces\" and \"Faces_easy\"\n",
    "keep_classes = [c for c in range(len(dataset.categories)) if c not in skip_classes]\n",
    "indices = list(range(len(dataset)))\n",
    "random.shuffle(indices)\n",
    "new_indices = []\n",
    "current_class_size = {class_number: 0 for class_number in np.unique(dataset.y)}\n",
    "for i in indices:\n",
    "    class_number = dataset.y[i]\n",
    "    if class_number not in skip_classes and current_class_size[class_number] < 100:\n",
    "        new_indices.append(i)\n",
    "        current_class_size[class_number] += 1\n",
    "indices = new_indices\n",
    "\n",
    "# Compute subset of labels given new indices selection\n",
    "labels = [translate_label(dataset.y[i], keep_classes) for i in indices]\n",
    "assert max(labels) == 98\n",
    "\n",
    "test_size = 640\n",
    "train_size = len(indices) - test_size\n",
    "train_idx, test_idx = train_test_split(\n",
    "    indices,\n",
    "    train_size=train_size,\n",
    "    test_size=test_size,\n",
    "    shuffle=True,\n",
    "    random_state=42,  # always get the same split\n",
    "    stratify=labels,\n",
    ")\n",
    "\n",
    "\n",
    "class DatasetSubsetWithTransform(Subset):\n",
    "    \n",
    "    def __init__(self, dataset, indices, transform=None):\n",
    "        super().__init__(dataset, indices)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = super().__getitem__(idx)\n",
    "        # Convert PIL image to RGB (some of them are greyscale)\n",
    "        x = x.convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "        y = translate_label(y, keep_classes)\n",
    "        return x, y\n",
    "    \n",
    "\n",
    "train_set = DatasetSubsetWithTransform(dataset, train_idx, train_transform)\n",
    "test_set = DatasetSubsetWithTransform(dataset, test_idx, test_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnIEkOhCBa5k"
   },
   "source": [
    "### A closer look at the dataset\n",
    "\n",
    "We first plot the size of each class and observe the class distribution is not uniform.\n",
    "\n",
    "Then we show random examples from the dataset, annotated with the class label and index.\n",
    "Note that the training images include standard augmentations typically used for vision models (defined above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MV8Th7V6eYx0",
    "outputId": "26bdc902-aa19-41b1-a2ad-54174c1095d1"
   },
   "outputs": [],
   "source": [
    "label_idxs, counts = np.unique(labels, return_counts=True)\n",
    "plt.figure(figsize=(20, 4.2))\n",
    "sns.barplot(x=[dataset.categories[keep_classes[label]] for label in label_idxs], y=counts, color=sns.color_palette('muted')[0])\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of examples\")\n",
    "plt.title(\"Class distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def show_dataset_examples(dataloader):\n",
    "    images, labels = next(iter(dataloader))\n",
    "    with sns.axes_style(\"white\"):\n",
    "      fig, axes = plt.subplots(4, 8, figsize=(16, 9.5))\n",
    "    axes = [ax for axes_ in axes for ax in axes_]   # flatten\n",
    "    for j, (img, label) in enumerate(zip(images[:32], labels[:32])):\n",
    "        plt.sca(axes[j])\n",
    "        show_image(img, title=f\"{dataset.categories[keep_classes[label.item()]]} ({label.item()})\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\\nTrain images (including augmentations):\")\n",
    "show_dataset_examples(train_loader)\n",
    "print(\"\\n\\nTest images:\")\n",
    "show_dataset_examples(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wt3BVFMF81TI"
   },
   "source": [
    "## Define a neural network\n",
    "\n",
    "**Assignment 1:** Adapt the CNN from the previous lab (CIFAR-10) to handle 224x224 images. We recommend reducing significantly the size of the tensors before flattening, by adding either convolutional layers with stride>1 or [MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) layers (but see e.g. also [AvgPool2d](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6VbF_R2IMrDT",
    "outputId": "62ddf5f5-2408-4aef-cc03-b0e95a11af36"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = ...   # Your code here!\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    \n",
    "model = Model(num_classes=len(np.unique(labels)))\n",
    "device = torch.device('cuda')  # use cuda or cpu\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-IUg3sq81TQ"
   },
   "source": [
    "## Define loss function and optimizer\n",
    "\n",
    "**Assignment 2:** Implement the criterion and optimizer, as in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48AX85QP81TR"
   },
   "outputs": [],
   "source": [
    "loss_fn = None  # Your code here!\n",
    "optimizer = None  # Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WneIN7C81TV"
   },
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iWT0ULctWvm1",
    "outputId": "632ea54a-8070-4310-bcaa-f1b9b0f354c4"
   },
   "outputs": [],
   "source": [
    "# Test the forward pass with dummy data\n",
    "out = model(torch.randn(2, 3, 224, 224).to(device))\n",
    "print(\"Output shape:\", out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "NkUanRRb81TW",
    "outputId": "f024acc3-a195-44e4-b4d9-9c512f6c05f7"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "validation_every_steps = 50\n",
    "\n",
    "step = 0\n",
    "model.train()\n",
    "\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "        \n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_accuracies_batches = []\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass.\n",
    "        output = model(inputs)\n",
    "        \n",
    "        # Compute loss.\n",
    "        loss = loss_fn(output, targets)\n",
    "        \n",
    "        # Clean up gradients from the model.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute gradients based on the loss from the current batch (backpropagation).\n",
    "        loss.backward()\n",
    "        \n",
    "        # Take one optimizer step using the gradients computed in the previous step.\n",
    "        optimizer.step()\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        # Compute accuracy.\n",
    "        predictions = output.max(1)[1]\n",
    "        train_accuracies_batches.append(accuracy(targets, predictions))\n",
    "        \n",
    "        if step % validation_every_steps == 0:\n",
    "            \n",
    "            # Append average training accuracy to list.\n",
    "            train_accuracies.append(np.mean(train_accuracies_batches))\n",
    "            \n",
    "            train_accuracies_batches = []\n",
    "        \n",
    "            # Compute accuracies on validation set.\n",
    "            valid_accuracies_batches = []\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                for inputs, targets in test_loader:\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    output = model(inputs)\n",
    "                    loss = loss_fn(output, targets)\n",
    "\n",
    "                    predictions = output.max(1)[1]\n",
    "\n",
    "                    # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=False).\n",
    "                    valid_accuracies_batches.append(accuracy(targets, predictions) * len(inputs))\n",
    "\n",
    "                model.train()\n",
    "                \n",
    "            # Append average validation accuracy to list.\n",
    "            valid_accuracies.append(np.sum(valid_accuracies_batches) / len(test_set))\n",
    "     \n",
    "            print(f\"Step {step:<5}   training accuracy: {train_accuracies[-1]}\")\n",
    "            print(f\"             test accuracy: {valid_accuracies[-1]}\")\n",
    "\n",
    "print(\"Finished training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qAsbC8I81Ta"
   },
   "source": [
    "## Test the network on the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Smv6_BwF81Ti",
    "outputId": "a65a986a-3497-4aa4-dd5f-3bedc949e84d"
   },
   "outputs": [],
   "source": [
    "# Evaluate test set\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    test_accuracies = []\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, targets)\n",
    "\n",
    "        predictions = output.max(1)[1]\n",
    "\n",
    "        # Multiply by len(x) because the final batch of DataLoader may be smaller (drop_last=True).\n",
    "        test_accuracies.append(accuracy(targets, predictions) * len(inputs))\n",
    "\n",
    "    test_accuracy = np.sum(test_accuracies) / len(test_set)\n",
    "    print(f\"Test accuracy: {test_accuracy:.3f}\")\n",
    "    \n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbn2kNxCRFoc"
   },
   "source": [
    "## Using a pre-trained model\n",
    "\n",
    "Here we will load a ResNet34 that was pre-trained on ImageNet. We then discard the linear classifier at the end of the network (the \"head\" of the network) and replace it with a new one that outputs the desired number of logits for classification. To get a rough idea of the structure of the model, we print it below.\n",
    "\n",
    "The argument `finetune_entire_model` in `initialize_model()` controls whether the entire pre-trained model is fine-tuned. When this is `False`, only the linear head is trained, and the rest of the model is fixed. The idea is that the features extracted by the ImageNet model, up to the final classification layer, are very informative also on other datasets (see, e.g., [this paper](https://arxiv.org/abs/1910.04867) on the transferability of deep representations in large vision models).\n",
    "\n",
    "We will start here by training only the linear head. You can experiment different models and variations.\n",
    "\n",
    "Below, we define the model and forget the one we just trained. After that, you can go back to the section \"Define loss function and optimizer\" and re-execute the notebook from there, to train and evaluate the new model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o4W_fQASeYx1",
    "outputId": "40878ae3-c3fa-41d3-af75-43f31822f115"
   },
   "outputs": [],
   "source": [
    "def initialize_model(model_name: str, *, num_classes: int, finetune_entire_model: bool = False):\n",
    "    \"\"\"Returns a pretrained model with a new last layer, and a dict with additional info.\n",
    "\n",
    "    The dict now contains the number of model parameters, computed as the number of\n",
    "    trainable parameters as soon as the model is loaded.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\n",
    "        f\"Loading model '{model_name}', with \"\n",
    "        f\"finetune_entire_model={finetune_entire_model}, changing the \"\n",
    "        f\"last layer to output {num_classes} logits.\"\n",
    "    )\n",
    "    model = timm.create_model(\n",
    "        model_name, pretrained=True, num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    num_model_parameters = 0\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            num_model_parameters += p.numel()\n",
    "\n",
    "    if not finetune_entire_model:\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Layer names are not consistent, so we have to consider a few cases. This might \n",
    "        # break for arbitrary models from timm (we have not checked all available models).\n",
    "        layer = None\n",
    "        if hasattr(model, \"classifier\"):\n",
    "            if isinstance(model.classifier, nn.Linear):\n",
    "                layer = model.classifier\n",
    "        elif hasattr(model, \"head\"):\n",
    "            if isinstance(model.head, nn.Linear):\n",
    "                layer = model.head\n",
    "            elif hasattr(model.head, \"fc\") and isinstance(model.head.fc, nn.Linear):\n",
    "                layer = model.head.fc\n",
    "            elif hasattr(model.head, \"l\") and isinstance(model.head.l, nn.Linear):\n",
    "                layer = model.head.l\n",
    "        elif hasattr(model, \"fc\"):\n",
    "            if isinstance(model.fc, nn.Linear):\n",
    "                layer = model.fc\n",
    "        if layer is None:\n",
    "            raise ValueError(f\"Couldn't automatically find last layer of model.\")\n",
    "        \n",
    "        # Make the last layer trainable.\n",
    "        layer.weight.requires_grad_()\n",
    "        layer.bias.requires_grad_()\n",
    "\n",
    "    num_trainable_model_parameters = 0\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            num_trainable_model_parameters += p.numel()\n",
    "\n",
    "    return model, {\n",
    "        \"num_model_parameters\": num_model_parameters,\n",
    "        \"num_trainable_model_parameters\": num_trainable_model_parameters,\n",
    "    }\n",
    "\n",
    "model, data = initialize_model('resnet34d', num_classes=len(np.unique(labels)), finetune_entire_model=False)\n",
    "\n",
    "print(model)\n",
    "print(\"Number of model parameters:\", data[\"num_model_parameters\"])\n",
    "print(\"Number of trainable parameters:\", data[\"num_trainable_model_parameters\"])\n",
    "\n",
    "device = torch.device('cuda')  # use cuda or cpu\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocnQOBAl81Tn"
   },
   "source": [
    "**Assignment 3:** \n",
    "\n",
    "1. Train the linear classifier on top of the pre-trained network, and observe how quickly you can get pretty good results, compared to training a smaller network from scratch as above.\n",
    "\n",
    "2. Go back and change argument to finetune entire network, maybe adjust learning rate, see if you can get better performance than before and if you run into any issues.\n",
    "\n",
    "3. Optional: experiment with `timm`: try smaller or larger models, including state-of-the-art models, e.g. based on vision transformers (ViT) or MLP-Mixers.\n",
    "\n",
    "4. Briefly describe what you did and any experiments you did along the way as well as what results you obtained.\n",
    "Did anything surprise you during the exercise?\n",
    "\n",
    "5. Write down key lessons/insights you got during this exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "4.3-CNN-transfer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
